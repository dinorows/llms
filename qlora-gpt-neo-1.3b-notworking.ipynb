{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc09d49-aee9-4035-9b6b-210088fe17cb",
   "metadata": {},
   "source": [
    "# 3. EleutherAI/gpt-neo-1.3b\n",
    "This is a *smaller* [model](https://huggingface.co/EleutherAI/gpt-neo-1.3B), also trained on [pile](https://pile.eleuther.ai/), an 825 GiB diverse, open source language modelling English text corpus targeted at training large-scale language models, whose weights should download faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31de5546-8a08-4ba9-a180-bb25b3bb11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefef7a6-54a2-41cc-b91c-d4323f584fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc744f8-f84a-4d29-b071-707582b27fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your GPU is not compatible with FlashAttention and bfloat16. :-(\n"
     ]
    }
   ],
   "source": [
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "if major_version >= 8:\n",
    "  !pip install flash-attn\n",
    "  torch_dtype = torch.bfloat16\n",
    "  attn_implementation='flash_attention_2'\n",
    "  print(\"Your GPU is compatible with FlashAttention and bfloat16. Yippeee :-)\")\n",
    "else:\n",
    "  torch_dtype = torch.float16\n",
    "  attn_implementation='eager'\n",
    "  print(\"Your GPU is not compatible with FlashAttention and bfloat16. :-(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e3ece7-3c95-4491-92de-75d6a0b8209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274cd85-28fe-416b-8160-0010fa8161b7",
   "metadata": {},
   "source": [
    "Now, we can load the model in 4-bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fa2075-d6ea-4020-a957-d8f91d3cfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             quantization_config=quant_config, \n",
    "                                             device_map={\"\":0}, \n",
    "                                             attn_implementation=attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6948db-47b4-464f-b313-48361964e45c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 2048)\n",
      "    (wpe): Embedding(2048, 2048)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "            (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "            (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720ff70-f001-476f-9a9c-45c7446016f5",
   "metadata": {},
   "source": [
    "Then, we enable gradient checkpointing and we use PEFT.\n",
    "\n",
    "We prepare the model for LoRA, adding trainable adapters for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b92cd4f-a8bb-4090-bf99-6fa02f47ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09570e3-f07e-453a-ab49-db15fa6a9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_peft = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3ec99-d434-4bf4-b07d-44c0c65e94a7",
   "metadata": {},
   "source": [
    "The first code section was targetted to `Gpt-NeoX-20b`, which has a single linear layer called `query_key_value` per Attention block.\n",
    "\n",
    "With the `Gpt-Neo-1.3b` model, we have 4 linear layers. I chose to fine-tune only the last linear layer, called `out_proj`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c18f13f-388e-416c-a33f-b081f273b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = LoraConfig(\n",
    "#    r=8, \n",
    "#    lora_alpha=32, \n",
    "#    target_modules=[\"query_key_value\"], \n",
    "#    lora_dropout=0.05, \n",
    "#    bias=\"none\", \n",
    "#    task_type=\"CAUSAL_LM\"\n",
    "#)\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model_lora = get_peft_model(model_peft, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc00cebe-d594-4be6-9244-4acbd12b62a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPTNeoForCausalLM(\n",
      "      (transformer): GPTNeoModel(\n",
      "        (wte): Embedding(50257, 2048)\n",
      "        (wpe): Embedding(2048, 2048)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-23): 24 x GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
      "              (c_proj): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d53dd21e-a3f3-422c-89ed-74fa37bb1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93017996-979c-4cda-8b4a-5f75771b42a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Faculty/dino/hf2/lib64/python3.12/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.use_cache = False\n",
    "trainer = transformers.Trainer(\n",
    "    model=model_lora,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        fp16 = torch_dtype == torch.float16,\n",
    "        bf16 = torch_dtype == torch.bfloat16,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir=\"trained_adapter/\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eb5b029-727f-40b3-a752-3b93828ec8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training at 2024-08-06 10:10:55.965630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Faculty/dino/hf2/lib64/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/Users/Faculty/dino/hf2/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/Faculty/dino/hf2/lib64/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/Users/Faculty/dino/hf2/lib64/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 1:49:36, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training at 2024-08-06 10:10:55.965630\n",
      "Training time: 1h 52m 21s\n"
     ]
    }
   ],
   "source": [
    "import time, datetime\n",
    "start = datetime.datetime.now()\n",
    "print(\"Started training at\", start)\n",
    "trainer.train()\n",
    "end = datetime.datetime.now()\n",
    "print(\"Finished training at\", start)\n",
    "diff = (end - start)\n",
    "diff_seconds = int(diff.total_seconds())\n",
    "minute_seconds, seconds = divmod(diff_seconds, 60)\n",
    "hours, minutes = divmod(minute_seconds, 60)\n",
    "hms = f\"{hours}h {minutes}m {seconds}s\"\n",
    "print(\"Training time:\", hms)\n",
    "\n",
    "# save the model now!\n",
    "torch.save(model_lora.state_dict(), \"/Users/Faculty/dino/qlora.gpt.neo.1.3b/weights.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45ebdc58-ae92-4377-b8f5-74a747a88afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/Users/Faculty/dino/hf2/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/Users/Faculty/dino/hf2/lib64/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask not what your country can do for you, but what you can do for your country.\n",
      "\n",
      "Menu\n",
      "\n",
      "Tag\n"
     ]
    }
   ],
   "source": [
    "text = \"Ask not what your country\"\n",
    "device = \"cuda\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model_lora.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5259e13-4057-4670-92a5-64b153f103c6",
   "metadata": {},
   "source": [
    "This is what the original model would have yielded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fadb7cba-e3e9-4820-aaaf-cf34234f6318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask not what your country can do for you, but what you can do for your country.\n",
      "\n",
      "Menu\n",
      "\n",
      "Tag\n"
     ]
    }
   ],
   "source": [
    "text = \"Ask not what your country\"\n",
    "device = \"cuda\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac879932-1fa8-42ee-8cb4-ca13ed8afbd6",
   "metadata": {},
   "source": [
    "Ok, no difference. This quote is very famous and was probably included in the pile.\n",
    "\n",
    "Let's try another one from the dataset of quotes: \n",
    "```\n",
    "“I'm not upset that you lied to me, I'm upset that from now on I can't believe you.” - F. Nietsche\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a45f327f-8e38-4dc7-9467-41fff73684e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not upset that you lied to me.\" \"I'm not upset that you lied to me.\" \"I'm not upset that you lied\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm not upset that you lied to me\"\n",
    "device = \"cuda\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model_lora.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea43b1d1-1449-42b5-adb2-2c19a80be6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not upset that you lied to me.\" \"I'm not upset that you lied to me.\" \"I'm not upset that you lied\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm not upset that you lied to me\"\n",
    "device = \"cuda\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dad8f75-ede0-4946-af28-3bd70d045daa",
   "metadata": {},
   "source": [
    "Hmm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53416b-e110-4ee6-9626-d834f1630a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
